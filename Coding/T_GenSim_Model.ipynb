{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11136368,"sourceType":"datasetVersion","datasetId":6945971},{"sourceId":337900,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":282601,"modelId":303466},{"sourceId":339308,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":283735,"modelId":304586},{"sourceId":339362,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":283788,"modelId":304634}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install bert_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T12:53:49.012655Z","iopub.execute_input":"2025-04-15T12:53:49.012854Z","iopub.status.idle":"2025-04-15T12:53:54.112460Z","shell.execute_reply.started":"2025-04-15T12:53:49.012835Z","shell.execute_reply":"2025-04-15T12:53:54.111296Z"}},"outputs":[{"name":"stdout","text":"Collecting bert_score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.5.1+cu121)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.47.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (24.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2025.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.29.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.5)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.7)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2025.1.31)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bert_score) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->bert_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->bert_score) (2024.2.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bert_score\nSuccessfully installed bert_score-0.3.13\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport random\nimport shutil\nfrom datasets import Dataset as HF_Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    pipeline\n)\nimport numpy as np\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom bert_score import score as bert_score\n\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ninput_file_path = \"/kaggle/input/dataset/conv_data/therapy_train/data-00000-of-00001.arrow\"\nwritable_file_path = \"/kaggle/working/train_data.arrow\"\nval_data_load_path = \"/kaggle/input/dataset/conv_data/therapy_val/data-00000-of-00001.arrow\"\nval_data_write_path = \"/kaggle/working/val_data.arrow\"\n\nshutil.copy(input_file_path, writable_file_path)\nshutil.copy(val_data_load_path, val_data_write_path)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T12:54:01.110619Z","iopub.execute_input":"2025-04-15T12:54:01.110920Z","iopub.status.idle":"2025-04-15T12:54:25.132765Z","shell.execute_reply.started":"2025-04-15T12:54:01.110895Z","shell.execute_reply":"2025-04-15T12:54:25.131890Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/val_data.arrow'"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom datasets import Dataset as HF_Dataset, load_from_disk\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    pipeline,\n    DataCollatorForSeq2Seq\n)\nimport numpy as np\nfrom accelerate import Accelerator\n\n\naccelerator = Accelerator()\nDEVICE = accelerator.device\n\nimport pyarrow as pa\nfrom datasets import Dataset\n\n\nimport torch\nimport pyarrow as pa\nfrom datasets import Dataset, load_dataset\n\n\ndef load_any_file(path):\n    \n    try:\n        \n        table = pa.ipc.open_file(pa.memory_map(path, 'r')).read_all()\n        return Dataset.from_arrow(table)\n    except pa.ArrowInvalid:\n        try:\n            table = pa.parquet.read_table(path)\n            return Dataset.from_arrow(table)\n        except:\n            return load_dataset('json', data_files=path)['train']\n    except Exception as e:\n        raise ValueError(f\"Unsupported file format for {path}: {str(e)}\")\n\n\ndef load_data_with_validation(input_path, output_path):\n    try:\n        dataset = load_any_file(input_path)\n        return dataset\n    except Exception as e:\n\n        print(f\"Error loading {input_path}: {e}, trying HF load_dataset\")\n        return load_dataset('arrow', data_files=input_path)['train']\n\n\ndataset = load_data_with_validation(input_file_path, writable_file_path)\nval_dataset = load_data_with_validation(val_data_load_path, val_data_write_path)\n\n\ndataset = dataset.with_format(\"torch\", device=DEVICE)\nval_dataset = val_dataset.with_format(\"torch\", device=DEVICE)\n\ndef build_triples_batched(dataset):\n    \n    def process_batch(batch):\n        \n        batch_size = len(batch[\"input_text\"])\n        return {\n            \n            \"history\": batch[\"input_text\"][:-1],\n            \"ut\": batch[\"target_text\"][:-1],\n            \"next_input\": batch[\"input_text\"][1:batch_size]\n        }\n    \n    processed = dataset.map(\n        process_batch,\n        batched=True,\n        batch_size=1024,\n        remove_columns=dataset.column_names\n    )\n    \n    def extract_ut1(batch):\n        return {\n            \"ut1\": [\n                next(\n                    (seg.split(\"P:\")[1].strip() for seg in text.split(\"[SEP]\") if \"P:\" in seg),\n                    None\n                )\n                for text in batch[\"next_input\"]\n            ]\n        }\n    \n    return processed.map(\n        extract_ut1,\n        batched=True,\n        batch_size=1024\n    ).filter(lambda x: x[\"ut1\"] is not None)\n\ntrain_triples = build_triples_batched(dataset)\nval_pairs = val_dataset.map(\n    lambda x: {\"history\": x[\"input_text\"], \"ut\": x[\"target_text\"]},\n    batched=True,\n    batch_size=1024\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T12:54:29.178180Z","iopub.execute_input":"2025-04-15T12:54:29.178819Z","iopub.status.idle":"2025-04-15T12:54:32.320266Z","shell.execute_reply.started":"2025-04-15T12:54:29.178791Z","shell.execute_reply":"2025-04-15T12:54:32.319361Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e723cd44553549ed80c08c33b5e8718f"}},"metadata":{}},{"name":"stderr","text":"Failed to load JSON from file '/kaggle/input/dataset/conv_data/therapy_train/data-00000-of-00001.arrow' with error <class 'pyarrow.lib.ArrowInvalid'>: JSON parse error: Invalid value. in row 0\n","output_type":"stream"},{"name":"stdout","text":"Error loading /kaggle/input/dataset/conv_data/therapy_train/data-00000-of-00001.arrow: An error occurred while generating the dataset, trying HF load_dataset\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e3f9bf971f14c35a283006bcd46b004"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4838177dbe0549eca5f13fd18a5f7086"}},"metadata":{}},{"name":"stderr","text":"Failed to load JSON from file '/kaggle/input/dataset/conv_data/therapy_val/data-00000-of-00001.arrow' with error <class 'pyarrow.lib.ArrowInvalid'>: JSON parse error: Invalid value. in row 0\n","output_type":"stream"},{"name":"stdout","text":"Error loading /kaggle/input/dataset/conv_data/therapy_val/data-00000-of-00001.arrow: An error occurred while generating the dataset, trying HF load_dataset\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d2f0b5826944e94806f1aa2acac406d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4008 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bdaa4671fca47f38fd9059ae9856874"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4004 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d62712319e2a490ca58119bcba3478cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/4004 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16b70d71413441338a413de4f2e17286"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/576 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"452c55c2f55744a8a8912a8b22681190"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"\n\nBASE_MODEL = \"t5-small\"\n\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n\n\ngenerator_model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL, torch_dtype=torch.float16).to(DEVICE)\nsimulator_model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL, torch_dtype=torch.float16).to(DEVICE)\nprint(\"models loaded\")\n\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8)  \nprint(\"data collaor done\")\n\ndef tokenize_function(batch):\n    \n    generator_inputs = tokenizer(\n        batch[\"history\"],\n        max_length=128,\n        truncation=True,\n        padding=\"max_length\",\n        return_tensors=\"pt\"\n    )\n    \n    # Generator targets\n    with tokenizer.as_target_tokenizer():\n        generator_targets = tokenizer(\n            batch[\"ut\"],\n            max_length=128,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n    \n    # Simulator inputs\n    simulator_texts = [f\"{h} [SEP] T: {ut}\" for h, ut in zip(batch[\"history\"], batch[\"ut\"])]\n    simulator_inputs = tokenizer(\n        simulator_texts,\n        max_length=128,\n        truncation=True,\n        padding=\"max_length\",\n        return_tensors=\"pt\"\n    )\n    \n    # Simulator targets\n    with tokenizer.as_target_tokenizer():\n        simulator_targets = tokenizer(\n            batch[\"ut1\"],\n            max_length=128,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n    \n    return {\n        \"generator_input_ids\": generator_inputs.input_ids,\n        \"generator_attention_mask\": generator_inputs.attention_mask,\n        \"generator_labels\": generator_targets.input_ids,\n        \"simulator_input_ids\": simulator_inputs.input_ids,\n        \"simulator_attention_mask\": simulator_inputs.attention_mask,\n        \"simulator_labels\": simulator_targets.input_ids,\n    }\n\n\nprint(\"create optimized dataloaders\")\ntrain_dataset = train_triples.map(\n    tokenize_function,\n    batched=True,\n    batch_size=1024,\n    remove_columns=train_triples.column_names\n)\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=16,\n    collate_fn=data_collator,\n    pin_memory=True,\n    num_workers=4\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T12:54:35.834645Z","iopub.execute_input":"2025-04-15T12:54:35.834960Z","iopub.status.idle":"2025-04-15T12:54:55.821019Z","shell.execute_reply.started":"2025-04-15T12:54:35.834937Z","shell.execute_reply":"2025-04-15T12:54:55.820107Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dc4abe9f3cf4818a0e43176d2f51ea1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f75e94a8099e4be5ad011d076c043559"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4f4d3f32fe54e43a4be87c6fe283c01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dae3782fbc84db1944b81c2d535b7ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e09d07c44464757b36045280be9d281"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f52ceb12c0d499fb9f368b301239ec0"}},"metadata":{}},{"name":"stdout","text":"models loaded\ndata collaor done\ncreate optimized dataloaders\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7dc977b73fa4eab87f31544feb5ae90"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# tokenize prparation\ndef tokenize_generator(batch):\n    \n    inputs = tokenizer(\n        batch[\"history\"],\n        max_length=128,\n        truncation=True,\n        padding=\"max_length\",\n        return_tensors=\"pt\"\n    )\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            batch[\"ut\"],\n            max_length=128,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n    return {\n        \"input_ids\": inputs.input_ids,\n        \"attention_mask\": inputs.attention_mask,\n        \"labels\": labels.input_ids\n    }\n\ndef tokenize_simulator(batch):\n    \n    inputs = tokenizer(\n        [f\"{h} [SEP] T: {ut}\" for h, ut in zip(batch[\"history\"], batch[\"ut\"])],\n        max_length=128,\n        truncation=True,\n        padding=\"max_length\",\n        return_tensors=\"pt\"\n    )\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            batch[\"ut1\"],\n            max_length=128,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n    return {\n        \"input_ids\": inputs.input_ids,\n        \"attention_mask\": inputs.attention_mask,\n        \"labels\": labels.input_ids\n    }\n\n\ngenerator_train_dataset = train_triples.map(\n    tokenize_generator,\n    batched=True,\n    batch_size=1024,\n    remove_columns=train_triples.column_names\n)\n\nsimulator_train_dataset = train_triples.map(\n    tokenize_simulator,\n    batched=True,\n    batch_size=1024,\n    remove_columns=train_triples.column_names\n)\n\n\ngenerator_train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\nsimulator_train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T12:55:20.344539Z","iopub.execute_input":"2025-04-15T12:55:20.344890Z","iopub.status.idle":"2025-04-15T12:55:20.371208Z","shell.execute_reply.started":"2025-04-15T12:55:20.344863Z","shell.execute_reply":"2025-04-15T12:55:20.370517Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\n\ndef train_model(model, tokenizer, dataset, output_dir):\n    \n    training_args = Seq2SeqTrainingArguments(\n        output_dir=output_dir,\n        learning_rate=3e-5,\n        per_device_train_batch_size=16,\n        gradient_accumulation_steps=2,\n        weight_decay=0.01,\n        save_total_limit=3,\n        num_train_epochs=3,\n        predict_with_generate=True,\n        fp16=torch.cuda.is_available(),  \n        dataloader_num_workers=4,\n        optim=\"adamw_torch\",\n        report_to=\"none\",\n        logging_steps=100,\n        save_strategy=\"steps\",\n        save_steps=500,\n        max_grad_norm=1.0  \n    )\n\n    \n    data_collator = DataCollatorForSeq2Seq(\n        tokenizer,\n        pad_to_multiple_of=8,\n        padding=True,\n        return_tensors=\"pt\"\n    )\n\n    \n    model = model.to(DEVICE, dtype=torch.float32)\n\n    \n    trainer = Seq2SeqTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=dataset,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n\n    print(\"Starting training...\")\n    trainer.train()\n    return model\n\n\ngenerator_train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\nsimulator_train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n# # Train models\nprint(\"Training Generator Model...\")\ngenerator_model = train_model(generator_model, tokenizer, generator_train_dataset, \"generator\")\n\nprint(\"\\nTraining Simulator Model...\")\nsimulator_model = train_model(simulator_model, tokenizer, simulator_train_dataset, \"simulator\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T12:55:28.066686Z","iopub.execute_input":"2025-04-15T12:55:28.066982Z","iopub.status.idle":"2025-04-15T12:59:51.414424Z","shell.execute_reply.started":"2025-04-15T12:55:28.066961Z","shell.execute_reply":"2025-04-15T12:59:51.413370Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-8-7da89d3d7065>:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [375/375 02:10, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>5.558600</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.054400</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.900600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n","output_type":"stream"},{"name":"stdout","text":"\nTraining Simulator Model...\nStarting training...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-8-7da89d3d7065>:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [375/375 02:09, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>5.510300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.248700</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.164700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# !zip -r saved_model.zip /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T21:55:34.209438Z","iopub.execute_input":"2025-04-14T21:55:34.209830Z","iopub.status.idle":"2025-04-14T21:56:50.923866Z","shell.execute_reply.started":"2025-04-14T21:55:34.209796Z","shell.execute_reply":"2025-04-14T21:56:50.922866Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/ (stored 0%)\n  adding: kaggle/working/val_data.arrow (deflated 96%)\n  adding: kaggle/working/.virtual_documents/ (stored 0%)\n  adding: kaggle/working/generator/ (stored 0%)\n  adding: kaggle/working/generator/checkpoint-375/ (stored 0%)\n  adding: kaggle/working/generator/checkpoint-375/training_args.bin (deflated 52%)\n  adding: kaggle/working/generator/checkpoint-375/config.json (deflated 62%)\n  adding: kaggle/working/generator/checkpoint-375/tokenizer_config.json (deflated 95%)\n  adding: kaggle/working/generator/checkpoint-375/trainer_state.json (deflated 58%)\n  adding: kaggle/working/generator/checkpoint-375/scheduler.pt (deflated 56%)\n  adding: kaggle/working/generator/checkpoint-375/optimizer.pt (deflated 7%)\n  adding: kaggle/working/generator/checkpoint-375/generation_config.json (deflated 29%)\n  adding: kaggle/working/generator/checkpoint-375/tokenizer.json (deflated 74%)\n  adding: kaggle/working/generator/checkpoint-375/model.safetensors (deflated 11%)\n  adding: kaggle/working/generator/checkpoint-375/special_tokens_map.json (deflated 85%)\n  adding: kaggle/working/generator/checkpoint-375/spiece.model (deflated 48%)\n  adding: kaggle/working/generator/checkpoint-375/rng_state.pth (deflated 25%)\n  adding: kaggle/working/train_data.arrow (deflated 97%)\n  adding: kaggle/working/simulator/ (stored 0%)\n  adding: kaggle/working/simulator/checkpoint-375/ (stored 0%)\n  adding: kaggle/working/simulator/checkpoint-375/training_args.bin (deflated 51%)\n  adding: kaggle/working/simulator/checkpoint-375/config.json (deflated 62%)\n  adding: kaggle/working/simulator/checkpoint-375/tokenizer_config.json (deflated 95%)\n  adding: kaggle/working/simulator/checkpoint-375/trainer_state.json (deflated 59%)\n  adding: kaggle/working/simulator/checkpoint-375/scheduler.pt (deflated 56%)\n  adding: kaggle/working/simulator/checkpoint-375/optimizer.pt (deflated 7%)\n  adding: kaggle/working/simulator/checkpoint-375/generation_config.json (deflated 29%)\n  adding: kaggle/working/simulator/checkpoint-375/tokenizer.json (deflated 74%)\n  adding: kaggle/working/simulator/checkpoint-375/model.safetensors (deflated 13%)\n  adding: kaggle/working/simulator/checkpoint-375/special_tokens_map.json (deflated 85%)\n  adding: kaggle/working/simulator/checkpoint-375/spiece.model (deflated 48%)\n  adding: kaggle/working/simulator/checkpoint-375/rng_state.pth (deflated 25%)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"print(len(train_triples))\nprint(train_triples[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T13:30:36.977587Z","iopub.execute_input":"2025-04-15T13:30:36.977871Z","iopub.status.idle":"2025-04-15T13:30:36.984606Z","shell.execute_reply.started":"2025-04-15T13:30:36.977851Z","shell.execute_reply":"2025-04-15T13:30:36.983695Z"}},"outputs":[{"name":"stdout","text":"4001\n{'history': 'T: Hi you how to do it today? [SEP] P: Great. How are you?', 'ut': \"I'm doing well. Thanks for asking.\", 'next_input': \"T: Hi you how to do it today? [SEP] P: Great. How are you? [SEP] T: I'm doing well. Thanks for asking.\", 'ut1': 'Great. How are you?'}\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\nimport torch\nfrom torch.utils.data import DataLoader\nfrom datasets import Dataset\nfrom tqdm import tqdm\n\n\nemotion_classifier = pipeline(\n    \"text-classification\",\n    model=\"bhadresh-savani/bert-base-uncased-emotion\",\n    device=0 if torch.cuda.is_available() else -1\n)\n\nclass TherapeuticTrainer:\n    def __init__(self, generator, simulator, tokenizer, emotion_classifier):\n        self.generator = generator.to(DEVICE)\n        self.simulator = simulator.to(DEVICE)\n        self.tokenizer = tokenizer\n        self.emotion_classifier = emotion_classifier\n        self.data_collator = DataCollatorForSeq2Seq(tokenizer)\n        \n        \n        for param in self.simulator.parameters():\n            param.requires_grad = False\n            \n        self.optimizer = torch.optim.AdamW(self.generator.parameters(), lr=3e-5)\n        self.scaler = torch.cuda.amp.GradScaler()\n\n    def calculate_reward(self, responses):\n        \n        batch_results = self.emotion_classifier(responses)\n        positive_emotions = {'joy', 'love', 'surprise'}\n        \n        \n        if isinstance(batch_results[0], dict):\n            batch_results = [batch_results]\n            \n        rewards = []\n        for result in batch_results:\n            if isinstance(result, list):\n                \n                score = sum(r['score'] if r['label'] in positive_emotions else -r['score'] for r in result)\n            else:\n                \n                score = 0.0\n            rewards.append(score)\n            \n        return torch.tensor(rewards, device=DEVICE)\n    def train_epoch(self, dataloader, num_candidates=5):\n        self.generator.train()\n        total_loss = torch.tensor(0.0, device=DEVICE)  \n        num_batches = len(dataloader)\n        \n        for batch in tqdm(dataloader):\n            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n            \n            with torch.cuda.amp.autocast():\n                \n                outputs = self.generator(**batch)\n                sup_loss = outputs.loss\n                \n                \n                gen_outputs = self.generator.generate(\n                    batch['input_ids'],\n                    max_length=128,\n                    num_beams=num_candidates,\n                    num_return_sequences=num_candidates,\n                    early_stopping=True\n                )\n                \n            \n            candidates = self.tokenizer.batch_decode(gen_outputs, skip_special_tokens=True)\n            original_inputs = self.tokenizer.batch_decode(batch['input_ids'], skip_special_tokens=True)\n            \n            \n            sim_inputs = [f\"{inp} [SEP] T: {cand}\" \n                        for inp, cand in zip(original_inputs * num_candidates, candidates)]\n            \n            with torch.no_grad():\n                sim_tokenized = self.tokenizer(\n                    sim_inputs, \n                    return_tensors='pt', \n                    padding=True, \n                    truncation=True,\n                    max_length=128\n                ).to(DEVICE)\n                \n                sim_outputs = self.simulator.generate(\n                    input_ids=sim_tokenized.input_ids,\n                    attention_mask=sim_tokenized.attention_mask,\n                    max_length=128,\n                    num_beams=3,\n                    early_stopping=True\n                )\n                simulated_responses = self.tokenizer.batch_decode(sim_outputs, skip_special_tokens=True)\n            \n            \n            rewards = torch.tensor([self.calculate_reward(r) for r in simulated_responses], \n                                 device=DEVICE).view(-1, num_candidates)\n            \n            # feedback learning is happeninig here\n            with torch.cuda.amp.autocast():\n                # Get dimensions\n                batch_size, seq_length, vocab_size = outputs.logits.shape\n                \n                # Expand log_probs for candidates\n                log_probs = torch.nn.functional.log_softmax(outputs.logits, dim=-1)\n                log_probs = log_probs.repeat_interleave(num_candidates, dim=0)\n                \n                # Expand rewards to match log_probs dimensions\n                expanded_rewards = rewards.view(-1, 1, 1).expand(\n                    -1, seq_length, vocab_size\n                )\n                \n                rl_loss = -torch.mean(log_probs * expanded_rewards)\n            \n            \n            batch_loss = 0.7 * sup_loss + 0.3 * rl_loss\n            total_loss += batch_loss  # Accumulate as tensor\n            \n            \n            self.optimizer.zero_grad()\n            self.scaler.scale(batch_loss).backward()\n            torch.nn.utils.clip_grad_norm_(self.generator.parameters(), 1.0)\n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n        \n        # Calculate average loss\n        avg_loss = total_loss / num_batches\n        return avg_loss.item()  \n\n    \n\n\ntokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer)\ntrain_loader = DataLoader(\n    generator_train_dataset,\n    batch_size=16,\n    collate_fn=data_collator,\n    shuffle=True\n)\n\n\ntrainer = TherapeuticTrainer(\n    generator_model,\n    simulator_model,\n    tokenizer,\n    emotion_classifier\n)\n\n\n\n\nfor epoch in range(3):\n    epoch_loss = trainer.train_epoch(train_loader)\n    print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n\n\ntrainer.generator.save_pretrained(\"/kaggle/working/final_therapeutic_model\")\ntokenizer.save_pretrained(\"/kaggle/working/final_therapeutic_model\")\nprint(\"model_saved_successfully\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T14:37:57.736606Z","iopub.execute_input":"2025-04-15T14:37:57.736915Z","iopub.status.idle":"2025-04-15T15:05:50.362656Z","shell.execute_reply.started":"2025-04-15T14:37:57.736894Z","shell.execute_reply":"2025-04-15T15:05:50.361714Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n<ipython-input-48-266de778e463>:27: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler()\n  0%|          | 0/225 [00:00<?, ?it/s]<ipython-input-48-266de778e463>:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n<ipython-input-48-266de778e463>:102: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 225/225 [27:51<00:00,  7.43s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Loss: 4.3705\nmodel_saved_successfully\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"# Training loop\nfor epoch in range(1,3):\n    epoch_loss = trainer.train_epoch(train_loader)\n    print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n\n\ntrainer.generator.save_pretrained(\"/kaggle/working/final2_therapeutic_model\")\ntokenizer.save_pretrained(\"/kaggle/working/final2_therapeutic_model\")\nprint(\"model_saved_successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:51:24.485633Z","iopub.execute_input":"2025-04-15T15:51:24.485976Z","iopub.status.idle":"2025-04-15T16:47:51.051314Z","shell.execute_reply.started":"2025-04-15T15:51:24.485947Z","shell.execute_reply":"2025-04-15T16:47:51.050123Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/225 [00:00<?, ?it/s]<ipython-input-48-266de778e463>:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n<ipython-input-48-266de778e463>:102: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 225/225 [28:27<00:00,  7.59s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Loss: 4.3438\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 225/225 [27:58<00:00,  7.46s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Loss: 4.3269\nmodel_saved_successfully\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":" \ndef therapeutic_response(context, max_length=128):\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n    inputs = tokenizer(\n        context,\n        return_tensors=\"pt\",\n        max_length=128,\n        truncation=True,\n        add_special_tokens=True  \n    ).to(device)\n    \n    outputs = generator_model.generate(\n        inputs.input_ids,\n        max_length=max_length,\n        min_length=10,\n        num_beams=5,\n        temperature=0.9,\n        early_stopping=True\n    )\n    \n    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return decoded if len(decoded) > 5 else \"[SAFETY] Could you elaborate?\"\n\n\n\ntest_dialogue = \"T: So, alright, let's take a step back and talk about competency. What education did you need for the job you have now? [SEP] P: I have my bachelor's degree. And we got a little bit of training. You know, every so often we get some training at work and stuff like that. [SEP]T: Are you evaluated at work by anybody to see if you re in a job you should be? [SEP] P: Yeah,I have a supervisor so they check up on stuff and also, like, if I feel like I have questions and stuff like that, I can go to them as well.\"\nprint(f\"Prediction is: {therapeutic_response(test_dialogue)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T14:13:08.791349Z","iopub.execute_input":"2025-04-15T14:13:08.791727Z","iopub.status.idle":"2025-04-15T14:13:09.173265Z","shell.execute_reply.started":"2025-04-15T14:13:08.791698Z","shell.execute_reply":"2025-04-15T14:13:09.172516Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Prediction is: Is that a job you're in? Is that a job you're in?\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\nfrom bert_score import score as bert_score\nfrom datasets import load_from_disk\nimport numpy as np\n\ndef evaluate_on_test_data(model_path, test_data_path, num_samples=None):\n    \n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(DEVICE)\n    \n    \n    test_dataset = load_from_disk(test_data_path)\n    \n    \n    if num_samples:\n        test_dataset = test_dataset.select(range(num_samples))\n    \n    \n    references = []\n    candidates = []\n    \n    print(f\"Evaluating on {len(test_dataset)} samples...\")\n    \n    for example in test_dataset:\n        \n        context = example[\"input_text\"]\n        generated = generate_response(model, tokenizer, context)\n        reference = example[\"target_text\"]\n        \n        \n        references.append([reference.split()])  \n        candidates.append(generated.split())\n    \n    \n    bleu = corpus_bleu(references, candidates)\n    \n    \n    cand_strings = [\" \".join(c) for c in candidates]\n    ref_strings = [\" \".join(r[0]) for r in references]\n    P, R, F1 = bert_score(cand_strings, ref_strings, lang=\"en\", device=DEVICE)\n    \n    return {\n        \"bleu\": bleu,\n        \"bert_precision\": np.mean(P.numpy()),\n        \"bert_recall\": np.mean(R.numpy()),\n        \"bert_f1\": np.mean(F1.numpy()),\n        \"sample_input\": context,\n        \"sample_reference\": reference,\n        \"sample_generated\": generated\n    }\n\n\ndef generate_response(model, tokenizer, context, max_length=128):\n    inputs = tokenizer(\n        context,\n        return_tensors=\"pt\",\n        max_length=128,\n        truncation=True\n    ).to(DEVICE)\n    \n    outputs = model.generate(\n        inputs.input_ids,\n        max_length=max_length,\n        num_beams=5,\n        early_stopping=True\n    )\n    \n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nUsage example\nresults = evaluate_on_test_data(\n    model_path=\"/kaggle/working/optimized_therapeutic_model\",\n    test_data_path=\"/kaggle/input/dataset/conv_data/therapy_test\"  \n)\n\nprint(f\"BLEU Score: {results['bleu']:.4f}\")\nprint(f\"BERT F1 Score: {results['bert_f1']:.4f}\")\nprint(f\"BERT Precision Score: {results['bert_precision']:.4f}\")\nprint(f\"BERT recall Score: {results['bert_recall']:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:08:34.715197Z","iopub.execute_input":"2025-04-15T17:08:34.715553Z","iopub.status.idle":"2025-04-15T17:08:34.725755Z","shell.execute_reply.started":"2025-04-15T17:08:34.715523Z","shell.execute_reply":"2025-04-15T17:08:34.724900Z"}},"outputs":[{"name":"stdout","text":"BLEU Score: 0.0253\nBERT F1 Score: 0.8151\nBERT Precision Score: 0.8154\nBERT recall Score: 0.8157\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"# load model\nimport torch\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nfrom datasets import load_from_disk\nmodel_path=\"/kaggle/working/final2_therapeutic_model\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:50:58.638272Z","iopub.execute_input":"2025-04-15T17:50:58.638606Z","iopub.status.idle":"2025-04-15T17:50:59.051717Z","shell.execute_reply.started":"2025-04-15T17:50:58.638582Z","shell.execute_reply":"2025-04-15T17:50:59.051028Z"}},"outputs":[],"execution_count":59}]}